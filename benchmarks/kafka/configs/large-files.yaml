# Benchmark configuration for Kafka - large files (chunked approach)
# Since Kafka has message size limits, we'll use 10MB chunks for comparison

# Kafka connection
kafka:
  brokers:
    - "localhost:9092"
  topic: "benchmark-large-files"
  producer:
    compression: "snappy"
    max_message_bytes: 104857600  # 100MB
    required_acks: 1
    timeout: "600s"
  consumer:
    group_id: "benchmark-consumer-group-large"
    session_timeout: "30s"
    heartbeat_interval: "3s"

# Test parameters
test:
  name: "large-files-chunked"
  message_size: 10485760  # 10MB per chunk (Kafka limit workaround)
  chunk_size: 10485760    # 10MB chunks
  target_file_size: 1073741824  # Simulate 1GB file (100 chunks)
  total_files: 10
  num_producers: 3
  num_consumers: 1
  target_rps: 0
  duration: "30m"
  warmup: "0s"

# Monitoring
monitoring:
  enabled: true
  interval: "10s"
  containers:
    - "benchmark-kafka"
    - "benchmark-zookeeper"

# Prometheus
prometheus:
  enabled: true
  pushgateway_url: "http://localhost:9091"
  push_interval: "10s"
  instance: "benchmark-1"

# Output
output:
  results_dir: "../../results/kafka"
  filename_prefix: "large-files-chunked"
  print_summary: true
  save_json: true
  save_csv: true
